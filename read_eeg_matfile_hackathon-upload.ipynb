{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.linalg import eig\n",
    "import matplotlib.pyplot as plt\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd 'C:\\\\projBCI\\\\training_example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_eeg_mat(filename, centered=True):\n",
    "    all_data = io.loadmat(filename)\n",
    "    eeg_data = all_data['data_cur']\n",
    "    if centered:\n",
    "        eeg_data = eeg_data - np.mean(eeg_data,1)[np.newaxis].T\n",
    "        print 'Data were centered: channels are zero-mean'\n",
    "    states_labels = all_data['states_cur']\n",
    "    states_codes = list(np.unique(states_labels)[:])\n",
    "    sampling_rate = all_data['srate']\n",
    "    chan_names = all_data['chan_names']\n",
    "    return eeg_data, states_labels, sampling_rate, chan_names, eeg_data.shape[0], eeg_data.shape[1], states_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, sampling_rate, order=5):\n",
    "    nyq_freq = sampling_rate*0.5\n",
    "    low = lowcut/nyq_freq\n",
    "    high = highcut/nyq_freq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_high_low_pass(lowcut, highcut, sampling_rate, order=5):\n",
    "    nyq_freq = sampling_rate*0.5\n",
    "    lower_bound = lowcut/nyq_freq\n",
    "    higher_bound = highcut/nyq_freq\n",
    "    b_high, a_high = butter(order, lower_bound, btype='high')\n",
    "    b_low, a_low = butter(order, higher_bound, btype='low')\n",
    "    return b_high, a_high, b_low, a_low\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, sampling_rate, order=5, how_to_filt = 'separately'):\n",
    "    if how_to_filt == 'separately':\n",
    "        b_high, a_high, b_low, a_low = butter_high_low_pass(lowcut, highcut, sampling_rate, order=order)\n",
    "        y = lfilter(b_high, a_high, data)\n",
    "        y = lfilter(b_low, a_low, y)\n",
    "    elif how_to_filt == 'simultaneously':\n",
    "        b, a = butter_bandpass(lowcut, highcut, sampling_rate, order=order)\n",
    "        y = lfilter(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_outliers(data_raw, states_labels_raw, iter_numb):\n",
    "    data = np.copy(data_raw)\n",
    "    states_labels = np.copy(states_labels_raw)\n",
    "    data_pwr = np.sqrt(np.sum(data**2,0))\n",
    "    \n",
    "    for i in range(iter_numb):\n",
    "        X_mean = np.mean(data_pwr)\n",
    "        X_std = np.std(data_pwr)\n",
    "        mask = np.abs(data_pwr - X_mean) < 2.5*np.abs(X_std)\n",
    "        data = data[:, mask]\n",
    "        states_labels = states_labels[:, mask]\n",
    "        data_pwr = data_pwr[mask]\n",
    "        print 'Samples left after outliers removal:', data_pwr.shape[0]\n",
    "        \n",
    "    return data, states_labels\n",
    "\n",
    "\n",
    "def remove_eog_simple(data, chan_names, eyechan, N_art_comp=3):\n",
    "    \n",
    "    only_eye_chan = data[chan_names[0,:]==eyechan,:]\n",
    "    exceed_mask = only_eye_chan > 3*np.mean(np.absolute(only_eye_chan))\n",
    "    print 'Number of samples identified as containing eye artifacts:', np.sum(exceed_mask)\n",
    "    U, S, V = np.linalg.svd(data[:, exceed_mask[0,:]], full_matrices=True)\n",
    "    M_eog = np.eye(U.shape[0])-np.dot(U[:,0:N_art_comp],U[:,0:N_art_comp].T)\n",
    "    \n",
    "    return np.dot(M_eog,data), M_eog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outer_n(n):\n",
    "    return np.array(list(range(n))+list(range(-n,0)))\n",
    "\n",
    "def whitener(C, rtol=1e-15):\n",
    "    e, E = np.linalg.eigh(C)\n",
    "    return reduce(np.dot, [E, np.diag(np.where(e > np.max(e) * rtol, e, np.inf)**-0.5), E.T])\n",
    "\n",
    "def csp_base(C_a, C_b):\n",
    "    P = whitener(C_a + C_b)\n",
    "    P_C_b = reduce(np.dot, [P, C_b, P.T])\n",
    "    _, _, B = np.linalg.svd((P_C_b))\n",
    "    return np.dot(B, P.T)\n",
    "\n",
    "def csp(C_a, C_b, m):\n",
    "    W = csp_base(C_a, C_b)\n",
    "    assert W.shape[1] >= 2*m\n",
    "    return W[outer_n(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_CSP_matr(data, states_labels, main_state, N_comp, other_state=None, mode='one_vs_all'):\n",
    "    \n",
    "    A = data[:, (states_labels == main_state)[0,:]]\n",
    "    if mode == 'one_vs_all':\n",
    "        B = data[:, (states_labels != main_state)[0,:]]\n",
    "    elif mode == 'pairwise':\n",
    "        if other_state == None:\n",
    "            print \"Other state must be specified\"\n",
    "            return None\n",
    "        else:\n",
    "            B = data[:, (states_labels == other_state)[0,:]]\n",
    "    \n",
    "    C1 = np.cov(A)\n",
    "    C2 = np.cov(B)\n",
    "    \n",
    "    return csp(C1,C2,N_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def const_features(data,states_labels,states_codes,sr,feat_type,freq_ranges,how_to_filt,N_csp_comp,win,order=5,normalize=False):\n",
    "    '''\n",
    "    Filters data according to specified bands (in freq_range) and derives CSP transformations for each band.\n",
    "    Type of CSP should be provided in feat_type: pairwise or one-vs-all (recommended).\n",
    "    Number of CSP components should be provided in N_csp_comp (for a given N, N first and N last components will be used).\n",
    "    Time interval for averaging is specified in win.\n",
    "    If normalize=True, each data point is in [0,1].\n",
    "    Returns array of transformed data, list of CSP transform matrices (in arrays), \n",
    "    and array of state codes for each of the final features (i.e., which state was first while this CSP projection was computed)\n",
    "    '''\n",
    "    final_data = np.zeros((1, data.shape[1]))\n",
    "    all_CSPs = []\n",
    "    where_states = []\n",
    "    \n",
    "    if feat_type == 'CSP_pairwise':\n",
    "        for freq in freq_ranges:\n",
    "            data_filt = butter_bandpass_filter(data, freq[0], freq[1], sr, order, how_to_filt)\n",
    "            all_states_CSP = []\n",
    "            for st in states_codes:\n",
    "                for oth_st in np.array(states_codes)[np.array(states_codes)!=st]:\n",
    "                    CSP_st = get_CSP_matr(data_filt, states_labels, st, N_csp_comp, other_state=oth_st, mode='pairwise')\n",
    "                    all_states_CSP.append(np.dot(CSP_st, data_filt))\n",
    "                    all_CSPs.append(CSP_st)\n",
    "                    where_states.extend([st]*(N_csp_comp*2))\n",
    "                data_transformed = np.vstack(all_states_CSP)**2\n",
    "            final_data = np.vstack((final_data, data_transformed))\n",
    "            \n",
    "    elif feat_type == 'CSP_one_vs_all':\n",
    "        for freq in freq_ranges:\n",
    "            data_filt = butter_bandpass_filter(data, freq[0], freq[1], sr, order, how_to_filt)\n",
    "            all_states_CSP = []\n",
    "            for st in states_codes:\n",
    "                CSP_st = get_CSP_matr(data_filt, states_labels, st, N_csp_comp, other_state=None, mode='one_vs_all')\n",
    "                all_states_CSP.append(np.dot(CSP_st, data_filt))\n",
    "                all_CSPs.append(CSP_st)\n",
    "                where_states.extend([st]*(N_csp_comp*2))\n",
    "            data_transformed = np.vstack(all_states_CSP)**2\n",
    "            final_data = np.vstack((final_data, data_transformed))\n",
    "            \n",
    "    elif feat_type == 'no_filt_no_csp':\n",
    "        final_data = np.vstack((final_data, data**2))\n",
    "\n",
    "    final_data = final_data[1:,:]\n",
    "    a_ma = 1\n",
    "    b_ma = np.ones(win)/float(win)\n",
    "    final_data = lfilter(b_ma, a_ma, final_data)\n",
    "    if normalize:\n",
    "        final_data = final_data/np.sum(final_data,0)[np.newaxis,:]\n",
    "    print 'Shape of data matrix:', final_data.shape\n",
    "        \n",
    "    return final_data, all_CSPs, np.array(where_states)\n",
    "\n",
    "def filt_apply_CSPs(data, sr, freq_range, all_CSPs, how_to_filt, win, order=5, normalize=False, no_filt_no_csp=False):\n",
    "    '''\n",
    "    Filters data according to specified bands (in freq_range) and applies corresponding CSP transformations (in all_CSPs).\n",
    "    Order in freq_range and all_CSPs must be the same.\n",
    "    If normalize=True, each data point is in [0,1].\n",
    "    '''\n",
    "    if no_filt_no_csp == False:\n",
    "        N_csp_per_freq = len(all_CSPs)/len(freq_range)\n",
    "        all_CSPs_copy = list(all_CSPs)\n",
    "        transformed_data = np.zeros((1, data.shape[1]))\n",
    "        for fr_ind in range(len(freq_range)):\n",
    "            filt_data = butter_bandpass_filter(data,freq_range[fr_ind][0],freq_range[fr_ind][1],sr,order,how_to_filt)\n",
    "            for csp_ind in range(N_csp_per_freq):\n",
    "                transformed_data = np.vstack((transformed_data, np.dot(all_CSPs_copy.pop(0), filt_data)))\n",
    "        final_data = transformed_data[1:,:]**2\n",
    "        \n",
    "    elif no_filt_no_csp == True:\n",
    "        final_data = data**2\n",
    "        \n",
    "    a_ma = 1\n",
    "    b_ma = np.ones(win)/float(win)\n",
    "    final_data = lfilter(b_ma, a_ma, final_data)\n",
    "    if normalize:\n",
    "        final_data = final_data/np.sum(final_data,0)[np.newaxis,:]\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load all data\n",
    "filename = 'alex_long_1'\n",
    "[eeg_data, states_labels, sampling_rate, chan_names, chan_numb, samp_numb, states_codes] = open_eeg_mat(filename, centered=False)\n",
    "sampling_rate = sampling_rate[0,0]\n",
    "\n",
    "# Prefilter eeg data\n",
    "eeg_data = butter_bandpass_filter(eeg_data, 0.5, 45, sampling_rate, order=5, how_to_filt = 'separately')\n",
    "\n",
    "# Remove empty channels\n",
    "nozeros_mask = np.sum(eeg_data[:,:sampling_rate*2],1)!=0 # Detect constant (zero) channels\n",
    "without_emp_mask = nozeros_mask & (chan_names[0,:]!='A1') & (chan_names[0,:]!='A2') & (chan_names[0,:]!='AUX')\n",
    "eeg_data = eeg_data[without_emp_mask,:] # Remove constant (zero) channels and prespecified channels\n",
    "chan_names_used = chan_names[:,without_emp_mask]\n",
    "\n",
    "# Remove outliers; remove artifacts (blinks, eye movements)\n",
    "eeg_data, states_labels = remove_outliers(eeg_data, states_labels, 7)\n",
    "eeg_data, M_eog = remove_eog_simple(eeg_data,chan_names_used,'Fp1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract CSP-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct features: project eeg data on CSP components (separately for each of specified frequency bands)\n",
    "N_CSP_comp = 3 # N first and N last; 2*N in total\n",
    "win = sampling_rate/2 # Window for averaging: 0.5 sec\n",
    "frequences = [(6,10),(8,12),(10,14),(12,16),(14,18),(16,20),(18,22),(20,24),\n",
    "              (22,26),(24,28),(26,30),(28,32),(30,34),(32,36),(34,38)]\n",
    "eeg_data, all_CSPs, where_states = const_features(eeg_data,states_labels,states_codes,sampling_rate,'CSP_one_vs_all',\n",
    "                                                    frequences,'separately',N_CSP_comp,win)\n",
    "\n",
    "eeg_data = eeg_data[:, win*2:]\n",
    "states_labels = states_labels[:, win*2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select features based on L1-regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tuned_parameters = [{'C': [0.0005]}]\n",
    "\n",
    "clf_l1_LR_1vsAll = GridSearchCV(LogisticRegression(penalty='l1'), tuned_parameters, cv=3)\n",
    "clf_l1_LR_2vsAll = GridSearchCV(LogisticRegression(penalty='l1'), tuned_parameters, cv=3)\n",
    "clf_l1_LR_6vsAll = GridSearchCV(LogisticRegression(penalty='l1'), tuned_parameters, cv=3)\n",
    "\n",
    "\n",
    "data_state_1 = eeg_data[(where_states==1), :]\n",
    "data_state_1 = data_state_1[:, (states_labels == 1)[0,:]]\n",
    "data_states_NOT_1 = eeg_data[(where_states==1), :]\n",
    "data_states_NOT_1 = data_states_NOT_1[:, (states_labels != 1)[0,:]]\n",
    "clf_l1_LR_1vsAll.fit(np.hstack((data_state_1, data_states_NOT_1)).T, \n",
    "                     np.vstack((np.ones((data_state_1.shape[1],1)),np.zeros((data_states_NOT_1.shape[1],1)))).ravel())\n",
    "\n",
    "data_state_2 = eeg_data[(where_states==2), :]\n",
    "data_state_2 = data_state_2[:, (states_labels == 2)[0,:]]\n",
    "data_states_NOT_2 = eeg_data[(where_states==2), :]\n",
    "data_states_NOT_2 = data_states_NOT_2[:, (states_labels != 2)[0,:]]\n",
    "clf_l1_LR_2vsAll.fit(np.hstack((data_state_2, data_states_NOT_2)).T, \n",
    "                    np.vstack((np.ones((data_state_2.shape[1],1)),np.zeros((data_states_NOT_2.shape[1],1)))).ravel())\n",
    "\n",
    "data_state_6 = eeg_data[(where_states==6), :]\n",
    "data_state_6 = data_state_6[:, (states_labels == 6)[0,:]]\n",
    "data_states_NOT_6 = eeg_data[(where_states==6), :]\n",
    "data_states_NOT_6 = data_states_NOT_6[:, (states_labels != 6)[0,:]]\n",
    "clf_l1_LR_6vsAll.fit(np.hstack((data_state_6, data_states_NOT_6)).T, \n",
    "                    np.vstack((np.ones((data_state_6.shape[1],1)),np.zeros((data_states_NOT_6.shape[1],1)))).ravel())\n",
    "\n",
    "all_coef_lasso = [clf_l1_LR_1vsAll.best_estimator_.coef_, clf_l1_LR_2vsAll.best_estimator_.coef_, \n",
    "                  clf_l1_LR_6vsAll.best_estimator_.coef_]\n",
    "for ind_coef_matr in range(len(all_coef_lasso)):\n",
    "    dummy_matr = all_coef_lasso[ind_coef_matr].reshape((len(frequences),N_CSP_comp*2))\n",
    "    print '\\n',states_codes[ind_coef_matr],'vs all:'\n",
    "    for i in range(len(frequences)):\n",
    "        if np.count_nonzero(dummy_matr[i,:])!=0:\n",
    "            print 'Frequency band', frequences[i], 'provided CSP components:', np.nonzero(dummy_matr[i,:])[0]+1\n",
    "            \n",
    "\n",
    "# Remove irrelevant (according to lasso LR) features \n",
    "masks_1 = np.split((clf_l1_LR_1vsAll.best_estimator_.coef_!=0), len(frequences), axis=1)\n",
    "masks_2 = np.split((clf_l1_LR_2vsAll.best_estimator_.coef_!=0), len(frequences), axis=1)\n",
    "masks_6 = np.split((clf_l1_LR_6vsAll.best_estimator_.coef_!=0), len(frequences), axis=1)\n",
    "masks_all_ordered = []\n",
    "for i in range(len(frequences)):\n",
    "    masks_all_ordered.extend(masks_1[i])\n",
    "    masks_all_ordered.extend(masks_2[i])\n",
    "    masks_all_ordered.extend(masks_6[i])\n",
    "    \n",
    "for j in range(len(masks_all_ordered)):\n",
    "    all_CSPs[j] = all_CSPs[j][masks_all_ordered[j],:]\n",
    "\n",
    "mask_data = np.hstack(masks_all_ordered)\n",
    "\n",
    "eeg_data = eeg_data[mask_data,:]\n",
    "print '\\nIrrelevant features have been removed'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "examples_numb = eeg_data.shape[1]\n",
    "train_labels = np.copy(states_labels)\n",
    "train_labels[train_labels==1] = 0\n",
    "train_labels[train_labels==2] = 1\n",
    "train_labels[train_labels==6] = 2\n",
    "\n",
    "pre_labels_mask = np.zeros((3, examples_numb))\n",
    "for row in range(3):\n",
    "     pre_labels_mask[row, :] = row\n",
    "one_of_K_labeled = np.zeros((3, examples_numb))\n",
    "for row in range(3):\n",
    "    one_of_K_labeled[row, :] = (pre_labels_mask[row, :] == train_labels)\n",
    "\n",
    "# Examples in rows, features in columns\n",
    "train_data = eeg_data.T\n",
    "train_labels = train_labels.T\n",
    "one_of_K_labeled = one_of_K_labeled.T\n",
    "print (train_data.shape,train_labels.shape,one_of_K_labeled.shape)\n",
    "\n",
    "input_size = train_data.shape[1]\n",
    "hidd_size = 15 # Number of units in hidden layer\n",
    "out_size = one_of_K_labeled.shape[1]\n",
    "print 'Input dim:', int(input_size), '\\nHidden dim:', hidd_size, '\\nOutput dim:', int(out_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric function (independent of classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_on_dataset(pred_labels, true_labels):\n",
    "    # pred_labels - list of predicted classes\n",
    "    # true_labels -  list of true classes\n",
    "    # Returns proportion of correct predictions\n",
    "    return np.sum(np.array(pred_labels)==np.array(true_labels))/float(len(true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define class for network with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NetOneHid:\n",
    "    \n",
    "    def __init__(self,input_size,hidd_size,out_size,how_init=\"auto_rand\",param_in=None):\n",
    "\n",
    "        self.X = theano.tensor.fmatrix('X')\n",
    "        self.Y = theano.tensor.imatrix('Y')\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        if how_init == \"auto_rand\":\n",
    "            self.W1_init = np.random.random((input_size,hidd_size)).astype('float32')*0.1\n",
    "            self.b1_init = np.random.random((hidd_size)).astype('float32')*0.1\n",
    "            self.W2_init = np.random.random((hidd_size,out_size)).astype('float32')*0.1\n",
    "            self.b2_init = np.random.random((out_size)).astype('float32')*0.1\n",
    "        elif how_init == \"by_hand\":\n",
    "            self.W1_init = param_in[0].astype('float32')\n",
    "            self.b1_init = param_in[1].astype('float32')\n",
    "            self.W2_init = param_in[2].astype('float32')\n",
    "            self.b2_init = param_in[3].astype('float32')\n",
    "\n",
    "        self.W1 = theano.shared(self.W1_init, name = 'W1')\n",
    "        self.b1 = theano.shared(self.b1_init, name = 'b1')\n",
    "        self.W2 = theano.shared(self.W2_init, name = 'W2')\n",
    "        self.b2 = theano.shared(self.b2_init, name = 'b2')\n",
    "\n",
    "        self.dot_1 = theano.tensor.dot(self.X, self.W1) + self.b1\n",
    "        self.activ_1 = theano.tensor.nnet.sigmoid(self.dot_1)\n",
    "        self.dot_2 = theano.tensor.dot(self.activ_1, self.W2) + self.b2\n",
    "        self.activ_final = theano.tensor.nnet.softmax(self.dot_2)\n",
    "\n",
    "        # Get the index of an output with the max activation (probability)\n",
    "        self.pred_y = theano.tensor.argmax(self.activ_final, axis=1)\n",
    "\n",
    "        # Define loss function\n",
    "        self.cross_loss = theano.tensor.nnet.categorical_crossentropy(self.activ_final, self.Y).mean()\n",
    "        \n",
    "        # Function to get classes' probabilities for a given input\n",
    "        self.pred_proba = theano.function([self.X], self.activ_final, allow_input_downcast = True)\n",
    "\n",
    "        # Function to predict class for a given input\n",
    "        self.predict_val = theano.function([self.X], self.pred_y, allow_input_downcast = True)\n",
    "    \n",
    "    def set_learning(self, lr_list, regul='no_reg', alpha_list=[]):\n",
    "        \n",
    "        if regul == 'no_reg':\n",
    "            self.final_loss = self.cross_loss\n",
    "        elif regul == 'L2':\n",
    "            self.reg_L2 = alpha_list[0]*(self.W1**2).sum() + alpha_list[1]*(self.W2**2).sum()\n",
    "            self.final_loss = self.cross_loss + self.reg_L2\n",
    "        \n",
    "        # Automatically find expressions for gradients\n",
    "        self.g_W2 = theano.tensor.grad(self.final_loss, self.W2)\n",
    "        self.g_b2 = theano.tensor.grad(self.final_loss, self.b2)\n",
    "        self.g_W1 = theano.tensor.grad(self.final_loss, self.W1)\n",
    "        self.g_b1 = theano.tensor.grad(self.final_loss, self.b1)\n",
    "        \n",
    "        #Set learning rates for weights and biases\n",
    "        self.lr_W1 = np.array(lr_list[0]).astype('float32')\n",
    "        self.lr_b1 = np.array(lr_list[1]).astype('float32')\n",
    "        self.lr_W2 = np.array(lr_list[2]).astype('float32')\n",
    "        self.lr_b2 = np.array(lr_list[3]).astype('float32')\n",
    "        \n",
    "        # Define how to update weights and biases\n",
    "        self.updates_for_params = [(self.W2, self.W2 - self.lr_W2*self.g_W2),\n",
    "                      (self.b2, self.b2 - self.lr_b2*self.g_b2),\n",
    "                      (self.W1, self.W1 - self.lr_W1*self.g_W1),\n",
    "                      (self.b1, self.b1 - self.lr_b1*self.g_b1)]\n",
    "\n",
    "        # Define theano function that trains network\n",
    "        self.train_net = theano.function(inputs = [self.X, self.Y],\n",
    "                           outputs = self.cross_loss,\n",
    "                           updates = self.updates_for_params,\n",
    "                           allow_input_downcast = True)\n",
    "    \n",
    "    def run_training(self,train_data,one_of_K_labeled,examples_numb,batch_size,max_epoch,verb_every_i=1,thr=0.0001):\n",
    "        \n",
    "        self.iter_numb = train_data.shape[0]//batch_size\n",
    "        self.all_mean_loss = []\n",
    "        \n",
    "        for epoch_ in xrange(max_epoch):\n",
    "            self.start_time = time.time()\n",
    "            self.loss_list = []\n",
    "            for iter_ in np.random.choice(np.arange(0,examples_numb), size=(self.iter_numb), replace=False):\n",
    "                self.loss_list.append(self.train_net(train_data[iter_:iter_+batch_size,:], \n",
    "                                                     one_of_K_labeled[iter_:iter_+batch_size,:]))\n",
    "            self.mean_loss = np.mean(self.loss_list)\n",
    "    \n",
    "            print 'Time passed (min):', (time.time()-self.start_time)/float(60)\n",
    "            if not epoch_ % verb_every_i:\n",
    "                print 'Epoch '+str(epoch_)+':\\naverage loss is '+str(self.mean_loss)+'\\n'\n",
    "\n",
    "            self.all_mean_loss.append(self.mean_loss)\n",
    "            if epoch_>0 and (self.all_mean_loss[-2]-self.all_mean_loss[-1])>0.0 and (self.all_mean_loss[-2]-self.all_mean_loss[-1])<thr:\n",
    "                break\n",
    "\n",
    "    # Function to check accuracy on a dataset (proportion of correct)\n",
    "    def accuracy_on_dataset(self, inputs, labels):\n",
    "        return sum([self.predict_val(inputs[i,:].reshape(1,inputs.shape[1])) == labels[i]\n",
    "                    for i in range(inputs.shape[0])])/float(inputs.shape[0])\n",
    "    \n",
    "    def get_probs(self, inputs):\n",
    "        return self.pred_proba(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "# Set network object and run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_1 = NetOneHid(input_size,hidd_size,out_size)\n",
    "\n",
    "net_1.set_learning([0.09,0.09,0.01,0.01], regul='no_reg')\n",
    "\n",
    "net_1.run_training(train_data,one_of_K_labeled,examples_numb,100,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check accuracy on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Train accuracy:', net_1.accuracy_on_dataset(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename_test = 'alex_long_2'\n",
    "\n",
    "[eeg_data_test,states_labels_test,sampling_rate_test,chan_names_test,chan_numb_test,samp_numb_test,states_codes_test] = open_eeg_mat(filename_test,\n",
    "                                                                                                                                     centered=False)\n",
    "\n",
    "eeg_data_test = butter_bandpass_filter(eeg_data_test, 0.5, 45, sampling_rate_test, order=5, how_to_filt='separately')\n",
    "eeg_data_test = eeg_data_test[without_emp_mask,:]\n",
    "chan_names_test_used = chan_names_test[:,without_emp_mask]\n",
    "eeg_data_test = np.dot(M_eog,eeg_data_test)\n",
    "\n",
    "eeg_data_test = filt_apply_CSPs(eeg_data_test, sampling_rate_test, frequences, all_CSPs, 'separately', win, no_filt_no_csp=False)\n",
    "\n",
    "\n",
    "test_data = eeg_data_test.T\n",
    "\n",
    "test_labels = np.copy(states_labels_test)\n",
    "test_labels[test_labels==1] = 0\n",
    "test_labels[test_labels==2] = 1\n",
    "test_labels[test_labels==6] = 2\n",
    "test_labels = test_labels.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Check accuracy on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Test accuracy:', net_1.accuracy_on_dataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define class for softmax (multiple class logistic) regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SoftmaxReg:\n",
    "    \n",
    "    def __init__(self, input_size, out_size, targ_type='int', how_init='auto_zeros', param_in=None):\n",
    "        \n",
    "        self.X = theano.tensor.fmatrix('X') # Number of examples (N) by dimentionality of outputs (M)\n",
    "        if targ_type==\"float\":\n",
    "            self.Y = theano.tensor.fmatrix('Y')\n",
    "        elif targ_type==\"int\":\n",
    "            self.Y = theano.tensor.imatrix('Y') \n",
    "        \n",
    "        if how_init == \"auto_zeros\":\n",
    "            self.param_init = np.zeros((input_size*out_size + out_size)).astype('float32')\n",
    "        elif how_init == \"auto_rand\":\n",
    "            self.param_init = np.random.random((input_size*out_size + out_size)).astype('float32')*0.1 \n",
    "        elif how_init == \"by_hand\":\n",
    "            self.param_init = param_in.astype('float32')\n",
    "        \n",
    "        self.parameters = theano.shared(self.param_init, name = 'parameters')\n",
    "        self.W = self.parameters[:input_size*out_size].reshape((input_size,out_size))\n",
    "        self.b = self.parameters[input_size*out_size:]\n",
    "        \n",
    "        self.p_y_given_x = theano.tensor.nnet.softmax(theano.tensor.dot(self.X, self.W) + self.b)\n",
    "        self.pred_y = theano.tensor.argmax(self.p_y_given_x, axis = 1)\n",
    "        \n",
    "        self.pred_proba = theano.function([self.X], self.p_y_given_x, allow_input_downcast = True)\n",
    "        self.predict_val = theano.function([self.X], self.pred_y, allow_input_downcast = True)\n",
    "        \n",
    "        # COST FUNCTION (WITH AVERAGING)\n",
    "        if targ_type==\"float\":\n",
    "            self.pre_cross_loss = (theano.tensor.log(self.p_y_given_x)*self.Y).sum(axis=1)\n",
    "            self.cross_loss = -(self.pre_cross_loss).mean()\n",
    "        elif targ_type==\"int\":\n",
    "            self.cross_loss = theano.tensor.nnet.categorical_crossentropy(self.p_y_given_x, self.Y).mean()\n",
    "            \n",
    "        \n",
    "    def set_learning(self, lr_list, regul='no_reg', alpha=0, opt_meth='GD'):\n",
    "        \n",
    "        if regul == 'no_reg':\n",
    "            self.final_loss = self.cross_loss\n",
    "        elif regul == 'L2':\n",
    "            self.reg_L2 = (self.W**2).sum()\n",
    "            self.final_loss = self.cross_loss + alpha*self.reg_L2\n",
    "        \n",
    "        self.lr_param = np.array(lr_list[0]).astype('float32')\n",
    "        self.g_param = theano.tensor.grad(self.final_loss, self.parameters)\n",
    "        \n",
    "        if opt_meth == 'GD':\n",
    "            self.updates_for_params = [(self.parameters, self.parameters - self.lr_param*self.g_param)]            \n",
    "        \n",
    "        # Output is cross_loss, but gradients are wrt final_loss\n",
    "        self.train_model = theano.function(inputs = [self.X, self.Y],\n",
    "                           outputs = self.cross_loss,\n",
    "                           updates = self.updates_for_params,\n",
    "                           allow_input_downcast = True)\n",
    "    \n",
    "    def run_training(self, inputs, targets, examples_numb, batch_size, max_epoch, test_data, test_labels, verb_every_i=1, thr=0.000001):\n",
    "        \n",
    "        self.iter_numb = inputs.shape[0]//batch_size\n",
    "        self.all_mean_loss = []\n",
    "        print \"\\nStarting model training...\"\n",
    "        for epoch_ in xrange(max_epoch):\n",
    "            \n",
    "            self.start_time = time.time()\n",
    "            self.loss_list = []\n",
    "            for iter_ in np.random.choice(np.arange(0,examples_numb), size=(self.iter_numb), replace=False):\n",
    "                self.loss_list.append(self.train_model(inputs[iter_:iter_+batch_size,:], targets[iter_:iter_+batch_size,:]))\n",
    "            self.mean_loss = np.mean(self.loss_list)\n",
    "            \n",
    "            if not epoch_ % verb_every_i:\n",
    "                print 'Time passed (min):', (time.time()-self.start_time)/float(60)\n",
    "                print 'Epoch '+str(epoch_)+':\\naverage loss is '+str(self.mean_loss)+'\\n'\n",
    "\n",
    "            self.all_mean_loss.append(self.mean_loss)\n",
    "            if epoch_>0 and (self.all_mean_loss[-2]-self.all_mean_loss[-1])>0.0 and (self.all_mean_loss[-2]-self.all_mean_loss[-1])<thr:\n",
    "                break\n",
    "                \n",
    "    def pred_stats_on_set(self, inputs, M_outs):\n",
    "        self.preds = self.predict_val(inputs)\n",
    "        self.freqs = [np.sum(self.preds==i) for i in range(M_outs)]\n",
    "        return {'individual choices':self.preds,'probabilities':self.pred_proba(inputs),'frequencies':self.freqs}\n",
    "    \n",
    "    def get_probs(self, inputs):\n",
    "        return self.pred_proba(inputs)\n",
    "    \n",
    "    def accuracy_on_dataset(self, inputs, labels):\n",
    "        return sum([self.predict_val(inputs[i,:].reshape(1,inputs.shape[1])) == labels[i]\n",
    "                    for i in range(inputs.shape[0])])/float(inputs.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
